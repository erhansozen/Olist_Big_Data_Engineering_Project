{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.s3.transfer import S3Transfer\n",
    "import boto3\n",
    "import os\n",
    "import zipfile\n",
    "from zipfile import ZipFile as zf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 access, bucket and file configurations\n",
    "aws_access_key_id = 'XXXX'\n",
    "aws_secret_access_key = 'XXXX'\n",
    "s3 = boto3.resource('s3')\n",
    "s3_bucket_name = 'erho2'\n",
    "file_downloaded = \"Olist.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from S3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(s3_bucket_name, file_downloaded, file_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_downloaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-70899cf28e84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Unzip dataset which I downloaded from S3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mzf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_downloaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mzipObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_downloaded' is not defined"
     ]
    }
   ],
   "source": [
    "# Unzip dataset which I downloaded from S3\n",
    "with zf(file_downloaded, 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset reading\n",
    "customer = pd.read_csv(\"olist_customers_dataset.csv\")\n",
    "geolocation = pd.read_csv(\"olist_geolocation_dataset.csv\")\n",
    "order_items = pd.read_csv(\"olist_order_items_dataset.csv\")\n",
    "order_payments = pd.read_csv(\"olist_order_payments_dataset.csv\")\n",
    "order_reviews = pd.read_csv(\"olist_order_reviews_dataset.csv\")\n",
    "orders = pd.read_csv(\"olist_orders_dataset.csv\")\n",
    "products = pd.read_csv(\"olist_products_dataset.csv\")\n",
    "sellers = pd.read_csv(\"olist_sellers_dataset.csv\")\n",
    "translation = pd.read_csv(\"product_category_name_translation.csv\") # We don't need it but I would like to read all of CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a0ffe_row0_col2, #T_a0ffe_row1_col2, #T_a0ffe_row3_col2 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a0ffe_row0_col3, #T_a0ffe_row0_col4, #T_a0ffe_row1_col3, #T_a0ffe_row1_col4, #T_a0ffe_row2_col3, #T_a0ffe_row2_col4, #T_a0ffe_row3_col3, #T_a0ffe_row3_col4, #T_a0ffe_row7_col2, #T_a0ffe_row7_col3, #T_a0ffe_row7_col4 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a0ffe_row2_col2, #T_a0ffe_row4_col2 {\n",
       "  background-color: #f2cbb7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a0ffe_row4_col3, #T_a0ffe_row6_col2, #T_a0ffe_row6_col4 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a0ffe_row4_col4 {\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a0ffe_row5_col2 {\n",
       "  background-color: #ee8468;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a0ffe_row5_col3 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a0ffe_row5_col4 {\n",
       "  background-color: #b9d0f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a0ffe_row6_col3 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a0ffe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a0ffe_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n",
       "      <th id=\"T_a0ffe_level0_col1\" class=\"col_heading level0 col1\" >Columns</th>\n",
       "      <th id=\"T_a0ffe_level0_col2\" class=\"col_heading level0 col2\" >Columns No</th>\n",
       "      <th id=\"T_a0ffe_level0_col3\" class=\"col_heading level0 col3\" >Null Amount</th>\n",
       "      <th id=\"T_a0ffe_level0_col4\" class=\"col_heading level0 col4\" >Null Columns Amount</th>\n",
       "      <th id=\"T_a0ffe_level0_col5\" class=\"col_heading level0 col5\" >Null Cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a0ffe_row0_col0\" class=\"data row0 col0\" >customers</td>\n",
       "      <td id=\"T_a0ffe_row0_col1\" class=\"data row0 col1\" >customer_id,customer_unique_id,customer_zip_code_prefix,customer_city,customer_state</td>\n",
       "      <td id=\"T_a0ffe_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_a0ffe_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_a0ffe_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_a0ffe_row0_col5\" class=\"data row0 col5\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a0ffe_row1_col0\" class=\"data row1 col0\" >geolocation</td>\n",
       "      <td id=\"T_a0ffe_row1_col1\" class=\"data row1 col1\" >geolocation_zip_code_prefix,geolocation_lat,geolocation_lng,geolocation_city,geolocation_state</td>\n",
       "      <td id=\"T_a0ffe_row1_col2\" class=\"data row1 col2\" >5</td>\n",
       "      <td id=\"T_a0ffe_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_a0ffe_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_a0ffe_row1_col5\" class=\"data row1 col5\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a0ffe_row2_col0\" class=\"data row2 col0\" >items</td>\n",
       "      <td id=\"T_a0ffe_row2_col1\" class=\"data row2 col1\" >order_id,order_item_id,product_id,seller_id,shipping_limit_date,price,freight_value</td>\n",
       "      <td id=\"T_a0ffe_row2_col2\" class=\"data row2 col2\" >7</td>\n",
       "      <td id=\"T_a0ffe_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_a0ffe_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_a0ffe_row2_col5\" class=\"data row2 col5\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a0ffe_row3_col0\" class=\"data row3 col0\" >payments</td>\n",
       "      <td id=\"T_a0ffe_row3_col1\" class=\"data row3 col1\" >order_id,payment_sequential,payment_type,payment_installments,payment_value</td>\n",
       "      <td id=\"T_a0ffe_row3_col2\" class=\"data row3 col2\" >5</td>\n",
       "      <td id=\"T_a0ffe_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_a0ffe_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "      <td id=\"T_a0ffe_row3_col5\" class=\"data row3 col5\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a0ffe_row4_col0\" class=\"data row4 col0\" >reviews</td>\n",
       "      <td id=\"T_a0ffe_row4_col1\" class=\"data row4 col1\" >review_id,order_id,review_score,review_comment_title,review_comment_message,review_creation_date,review_answer_timestamp</td>\n",
       "      <td id=\"T_a0ffe_row4_col2\" class=\"data row4 col2\" >7</td>\n",
       "      <td id=\"T_a0ffe_row4_col3\" class=\"data row4 col3\" >145903</td>\n",
       "      <td id=\"T_a0ffe_row4_col4\" class=\"data row4 col4\" >2</td>\n",
       "      <td id=\"T_a0ffe_row4_col5\" class=\"data row4 col5\" >review_comment_title, review_comment_message</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a0ffe_row5_col0\" class=\"data row5 col0\" >orders</td>\n",
       "      <td id=\"T_a0ffe_row5_col1\" class=\"data row5 col1\" >order_id,customer_id,order_status,order_purchase_timestamp,order_approved_at,order_delivered_carrier_date,order_delivered_customer_date,order_estimated_delivery_date</td>\n",
       "      <td id=\"T_a0ffe_row5_col2\" class=\"data row5 col2\" >8</td>\n",
       "      <td id=\"T_a0ffe_row5_col3\" class=\"data row5 col3\" >4908</td>\n",
       "      <td id=\"T_a0ffe_row5_col4\" class=\"data row5 col4\" >3</td>\n",
       "      <td id=\"T_a0ffe_row5_col5\" class=\"data row5 col5\" >order_approved_at, order_delivered_carrier_date, order_delivered_customer_date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a0ffe_row6_col0\" class=\"data row6 col0\" >products</td>\n",
       "      <td id=\"T_a0ffe_row6_col1\" class=\"data row6 col1\" >product_id,product_category_name,product_name_lenght,product_description_lenght,product_photos_qty,product_weight_g,product_length_cm,product_height_cm,product_width_cm</td>\n",
       "      <td id=\"T_a0ffe_row6_col2\" class=\"data row6 col2\" >9</td>\n",
       "      <td id=\"T_a0ffe_row6_col3\" class=\"data row6 col3\" >2448</td>\n",
       "      <td id=\"T_a0ffe_row6_col4\" class=\"data row6 col4\" >8</td>\n",
       "      <td id=\"T_a0ffe_row6_col5\" class=\"data row6 col5\" >product_category_name, product_name_lenght, product_description_lenght, product_photos_qty, product_weight_g, product_length_cm, product_height_cm, product_width_cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a0ffe_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a0ffe_row7_col0\" class=\"data row7 col0\" >sellers</td>\n",
       "      <td id=\"T_a0ffe_row7_col1\" class=\"data row7 col1\" >seller_id,seller_zip_code_prefix,seller_city,seller_state</td>\n",
       "      <td id=\"T_a0ffe_row7_col2\" class=\"data row7 col2\" >4</td>\n",
       "      <td id=\"T_a0ffe_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_a0ffe_row7_col4\" class=\"data row7 col4\" >0</td>\n",
       "      <td id=\"T_a0ffe_row7_col5\" class=\"data row7 col5\" ></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11aa40d44f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset information\n",
    "datasets = [customer, geolocation, order_items, order_payments, order_reviews, orders, products, sellers]\n",
    "titles = [\"customers\",\"geolocation\", \"items\", \"payments\", \"reviews\", \"orders\", \"products\", \"sellers\"]\n",
    "info_df = pd.DataFrame({},)\n",
    "info_df[\"Dataset\"] = titles\n",
    "info_df['Columns'] = [','.join([col for col, null in df.isnull().sum().items()]) for df in datasets]\n",
    "info_df[\"Columns No\"] = [df.shape[1] for df in datasets]\n",
    "info_df[\"Null Amount\"] = [df.isnull().sum().sum() for df in datasets]\n",
    "info_df[\"Null Columns Amount\"] = [len([col for col, null in df.isnull().sum().items() if null > 0]) for df in datasets]\n",
    "info_df['Null Cols'] = [', '.join([col for col, null in df.isnull().sum().items() if null > 0]) for df in datasets]\n",
    "\n",
    "info_df.style.background_gradient(cmap='coolwarm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 113367 entries, 0 to 115877\n",
      "Data columns (total 31 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   order_id                       113367 non-null  object \n",
      " 1   customer_id                    113367 non-null  object \n",
      " 2   order_status                   113367 non-null  object \n",
      " 3   order_purchase_timestamp       113367 non-null  object \n",
      " 4   order_approved_at              113367 non-null  object \n",
      " 5   order_delivered_carrier_date   113367 non-null  object \n",
      " 6   order_delivered_customer_date  113367 non-null  object \n",
      " 7   order_estimated_delivery_date  113367 non-null  object \n",
      " 8   payment_sequential             113367 non-null  int64  \n",
      " 9   payment_type                   113367 non-null  object \n",
      " 10  payment_installments           113367 non-null  int64  \n",
      " 11  payment_value                  113367 non-null  float64\n",
      " 12  customer_unique_id             113367 non-null  object \n",
      " 13  customer_zip_code_prefix       113367 non-null  int64  \n",
      " 14  customer_city                  113367 non-null  object \n",
      " 15  customer_state                 113367 non-null  object \n",
      " 16  order_item_id                  113367 non-null  int64  \n",
      " 17  product_id                     113367 non-null  object \n",
      " 18  seller_id                      113367 non-null  object \n",
      " 19  shipping_limit_date            113367 non-null  object \n",
      " 20  price                          113367 non-null  float64\n",
      " 21  freight_value                  113367 non-null  float64\n",
      " 22  product_category_name          113367 non-null  object \n",
      " 23  product_name_lenght            113367 non-null  float64\n",
      " 24  product_description_lenght     113367 non-null  float64\n",
      " 25  product_photos_qty             113367 non-null  float64\n",
      " 26  product_weight_g               113367 non-null  float64\n",
      " 27  product_length_cm              113367 non-null  float64\n",
      " 28  product_height_cm              113367 non-null  float64\n",
      " 29  product_width_cm               113367 non-null  float64\n",
      " 30  product_category_name_english  113367 non-null  object \n",
      "dtypes: float64(10), int64(4), object(17)\n",
      "memory usage: 27.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Merge all tables and check missing values and types of datas\n",
    "df = pd.merge(orders,order_payments, on=\"order_id\")\n",
    "df = df.merge(customer, on=\"customer_id\")\n",
    "df = df.merge(order_items, on=\"order_id\")\n",
    "df = df.merge(products, on=\"product_id\")\n",
    "df = df.merge(translation, on=\"product_category_name\")\n",
    "df.dropna(inplace=True) # Null variables have gone\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction data types of dates\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df.order_purchase_timestamp)\n",
    "df['order_approved_at'] = pd.to_datetime(df.order_approved_at)\n",
    "df['order_delivered_carrier_date'] = pd.to_datetime(df.order_delivered_carrier_date)\n",
    "df['order_delivered_customer_date'] = pd.to_datetime(df.order_delivered_customer_date)\n",
    "df['order_estimated_delivery_date'] = pd.to_datetime(df.order_estimated_delivery_date)\n",
    "df['shipping_limit_date'] = pd.to_datetime(df.shipping_limit_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 113367 entries, 0 to 115877\n",
      "Data columns (total 31 columns):\n",
      " #   Column                         Non-Null Count   Dtype         \n",
      "---  ------                         --------------   -----         \n",
      " 0   order_id                       113367 non-null  object        \n",
      " 1   customer_id                    113367 non-null  object        \n",
      " 2   order_status                   113367 non-null  object        \n",
      " 3   order_purchase_timestamp       113367 non-null  datetime64[ns]\n",
      " 4   order_approved_at              113367 non-null  datetime64[ns]\n",
      " 5   order_delivered_carrier_date   113367 non-null  datetime64[ns]\n",
      " 6   order_delivered_customer_date  113367 non-null  datetime64[ns]\n",
      " 7   order_estimated_delivery_date  113367 non-null  datetime64[ns]\n",
      " 8   payment_sequential             113367 non-null  int64         \n",
      " 9   payment_type                   113367 non-null  object        \n",
      " 10  payment_installments           113367 non-null  int64         \n",
      " 11  payment_value                  113367 non-null  float64       \n",
      " 12  customer_unique_id             113367 non-null  object        \n",
      " 13  customer_zip_code_prefix       113367 non-null  int64         \n",
      " 14  customer_city                  113367 non-null  object        \n",
      " 15  customer_state                 113367 non-null  object        \n",
      " 16  order_item_id                  113367 non-null  int64         \n",
      " 17  product_id                     113367 non-null  object        \n",
      " 18  seller_id                      113367 non-null  object        \n",
      " 19  shipping_limit_date            113367 non-null  datetime64[ns]\n",
      " 20  price                          113367 non-null  float64       \n",
      " 21  freight_value                  113367 non-null  float64       \n",
      " 22  product_category_name          113367 non-null  object        \n",
      " 23  product_name_lenght            113367 non-null  float64       \n",
      " 24  product_description_lenght     113367 non-null  float64       \n",
      " 25  product_photos_qty             113367 non-null  float64       \n",
      " 26  product_weight_g               113367 non-null  float64       \n",
      " 27  product_length_cm              113367 non-null  float64       \n",
      " 28  product_height_cm              113367 non-null  float64       \n",
      " 29  product_width_cm               113367 non-null  float64       \n",
      " 30  product_category_name_english  113367 non-null  object        \n",
      "dtypes: datetime64[ns](6), float64(10), int64(4), object(11)\n",
      "memory usage: 27.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge \"id\" column from orders table by order_id column, \n",
    "# Merge \"product_id\", \"product_category_name\", \"product_id\" columns from products table by product_id column\n",
    "# Merged these tables for calculate late orders\n",
    "merged_order_products = order_items.merge(orders, on=\"order_id\").merge(products[[\"product_id\", \"product_category_name\"]], on=\"product_id\")\n",
    "late_orders = merged_order_products.loc[merged_order_products.shipping_limit_date < merged_order_products.order_delivered_carrier_date].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "      <th>product_category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.9</td>\n",
       "      <td>13.29</td>\n",
       "      <td>3ce436f183e68e07877b285a838db11a</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-09-13 08:59:02</td>\n",
       "      <td>2017-09-13 09:45:35</td>\n",
       "      <td>2017-09-19 18:34:16</td>\n",
       "      <td>2017-09-20 23:43:48</td>\n",
       "      <td>2017-09-29 00:00:00</td>\n",
       "      <td>cool_stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130898c0987d1801452a8ed92a670612</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-07-05 02:44:11</td>\n",
       "      <td>55.9</td>\n",
       "      <td>17.96</td>\n",
       "      <td>e6eecc5a77de221464d1c4eaff0a9b64</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-06-28 11:52:20</td>\n",
       "      <td>2017-06-29 02:44:11</td>\n",
       "      <td>2017-07-05 12:00:33</td>\n",
       "      <td>2017-07-13 20:39:29</td>\n",
       "      <td>2017-07-26 00:00:00</td>\n",
       "      <td>cool_stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>532ed5e14e24ae1f0d735b91524b98b9</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2018-05-23 10:56:25</td>\n",
       "      <td>64.9</td>\n",
       "      <td>18.33</td>\n",
       "      <td>4ef55bf80f711b372afebcb7c715344a</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-05-18 10:25:53</td>\n",
       "      <td>2018-05-18 12:31:43</td>\n",
       "      <td>2018-05-23 14:05:00</td>\n",
       "      <td>2018-06-04 18:34:26</td>\n",
       "      <td>2018-06-07 00:00:00</td>\n",
       "      <td>cool_stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13</td>\n",
       "      <td>239.9</td>\n",
       "      <td>19.93</td>\n",
       "      <td>f6dd3ec061db4e3987629fe6b26e5cce</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-04-26 10:53:06</td>\n",
       "      <td>2017-04-26 11:05:13</td>\n",
       "      <td>2017-05-04 14:35:00</td>\n",
       "      <td>2017-05-12 16:04:24</td>\n",
       "      <td>2017-05-15 00:00:00</td>\n",
       "      <td>pet_shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "      <td>2017-02-13 13:57:51</td>\n",
       "      <td>199.9</td>\n",
       "      <td>18.14</td>\n",
       "      <td>58dbd0b2d70206bf40e62cd34e84d795</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-02-04 13:57:51</td>\n",
       "      <td>2017-02-04 14:10:13</td>\n",
       "      <td>2017-02-16 09:46:09</td>\n",
       "      <td>2017-03-01 16:42:31</td>\n",
       "      <td>2017-03-17 00:00:00</td>\n",
       "      <td>ferramentas_jardim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            order_id  order_item_id  \\\n",
       "0   00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1   130898c0987d1801452a8ed92a670612              1   \n",
       "2   532ed5e14e24ae1f0d735b91524b98b9              1   \n",
       "9   00018f77f2f0320c557190d7a144bdd3              1   \n",
       "15  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
       "\n",
       "                          product_id                         seller_id  \\\n",
       "0   4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1   4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "2   4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "9   e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "15  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       "\n",
       "    shipping_limit_date  price  freight_value  \\\n",
       "0   2017-09-19 09:45:35   58.9          13.29   \n",
       "1   2017-07-05 02:44:11   55.9          17.96   \n",
       "2   2018-05-23 10:56:25   64.9          18.33   \n",
       "9   2017-05-03 11:05:13  239.9          19.93   \n",
       "15  2017-02-13 13:57:51  199.9          18.14   \n",
       "\n",
       "                         customer_id order_status order_purchase_timestamp  \\\n",
       "0   3ce436f183e68e07877b285a838db11a    delivered      2017-09-13 08:59:02   \n",
       "1   e6eecc5a77de221464d1c4eaff0a9b64    delivered      2017-06-28 11:52:20   \n",
       "2   4ef55bf80f711b372afebcb7c715344a    delivered      2018-05-18 10:25:53   \n",
       "9   f6dd3ec061db4e3987629fe6b26e5cce    delivered      2017-04-26 10:53:06   \n",
       "15  58dbd0b2d70206bf40e62cd34e84d795    delivered      2017-02-04 13:57:51   \n",
       "\n",
       "      order_approved_at order_delivered_carrier_date  \\\n",
       "0   2017-09-13 09:45:35          2017-09-19 18:34:16   \n",
       "1   2017-06-29 02:44:11          2017-07-05 12:00:33   \n",
       "2   2018-05-18 12:31:43          2018-05-23 14:05:00   \n",
       "9   2017-04-26 11:05:13          2017-05-04 14:35:00   \n",
       "15  2017-02-04 14:10:13          2017-02-16 09:46:09   \n",
       "\n",
       "   order_delivered_customer_date order_estimated_delivery_date  \\\n",
       "0            2017-09-20 23:43:48           2017-09-29 00:00:00   \n",
       "1            2017-07-13 20:39:29           2017-07-26 00:00:00   \n",
       "2            2018-06-04 18:34:26           2018-06-07 00:00:00   \n",
       "9            2017-05-12 16:04:24           2017-05-15 00:00:00   \n",
       "15           2017-03-01 16:42:31           2017-03-17 00:00:00   \n",
       "\n",
       "   product_category_name  \n",
       "0             cool_stuff  \n",
       "1             cool_stuff  \n",
       "2             cool_stuff  \n",
       "9               pet_shop  \n",
       "15    ferramentas_jardim  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Late orders list\n",
    "late_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we can use this commands provide types of data for each column BUT I don't know why it gives me a new baby column named \"c0\" xD You can check with codes below\n",
    "# late_orders.to_csv(\"late_orders.csv\")\n",
    "# spark_late = sc.read.csv(\"late_orders.csv\", inferSchema=True, header=True)\n",
    "# spark_late.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Bigdata\\spark\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[1]\") \\\n",
    ".appName(\"\") \\\n",
    ".getOrCreate()\n",
    "sql = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+---------------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|product_category_name|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+---------------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|3ce436f183e68e078...|   delivered|     2017-09-13 08:59:02|2017-09-13 09:45:35|         2017-09-19 18:34:16|          2017-09-20 23:43:48|          2017-09-29 00:00:00|           cool_stuff|\n",
      "|130898c0987d18014...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-07-05 02:44:11| 55.9|        17.96|e6eecc5a77de22146...|   delivered|     2017-06-28 11:52:20|2017-06-29 02:44:11|         2017-07-05 12:00:33|          2017-07-13 20:39:29|          2017-07-26 00:00:00|           cool_stuff|\n",
      "|532ed5e14e24ae1f0...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2018-05-23 10:56:25| 64.9|        18.33|4ef55bf80f711b372...|   delivered|     2018-05-18 10:25:53|2018-05-18 12:31:43|         2018-05-23 14:05:00|          2018-06-04 18:34:26|          2018-06-07 00:00:00|           cool_stuff|\n",
      "|6f8c31653edb8c83e...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-08-07 18:55:08| 58.9|        16.17|30407a72ad8b3f4df...|   delivered|     2017-08-01 18:38:42|2017-08-01 18:55:08|         2017-08-02 19:07:36|          2017-08-09 21:26:33|          2017-08-25 00:00:00|           cool_stuff|\n",
      "|7d19f4ef4d0446198...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-08-16 22:05:11| 58.9|        13.29|91a792fef70ecd8cc...|   delivered|     2017-08-10 21:48:40|2017-08-10 22:05:11|         2017-08-11 19:43:07|          2017-08-24 20:04:21|          2017-09-01 00:00:00|           cool_stuff|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark_late_items = sql.createDataFrame(merged_order_products)\n",
    "spark_late_items.createOrReplaceTempView('spark_late_items')\n",
    "spark_late_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+---------------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|product_category_name|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+---------------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|3ce436f183e68e078...|   delivered|     2017-09-13 08:59:02|2017-09-13 09:45:35|         2017-09-19 18:34:16|          2017-09-20 23:43:48|          2017-09-29 00:00:00|           cool_stuff|\n",
      "|130898c0987d18014...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-07-05 02:44:11| 55.9|        17.96|e6eecc5a77de22146...|   delivered|     2017-06-28 11:52:20|2017-06-29 02:44:11|         2017-07-05 12:00:33|          2017-07-13 20:39:29|          2017-07-26 00:00:00|           cool_stuff|\n",
      "|532ed5e14e24ae1f0...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2018-05-23 10:56:25| 64.9|        18.33|4ef55bf80f711b372...|   delivered|     2018-05-18 10:25:53|2018-05-18 12:31:43|         2018-05-23 14:05:00|          2018-06-04 18:34:26|          2018-06-07 00:00:00|           cool_stuff|\n",
      "|6f8c31653edb8c83e...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-08-07 18:55:08| 58.9|        16.17|30407a72ad8b3f4df...|   delivered|     2017-08-01 18:38:42|2017-08-01 18:55:08|         2017-08-02 19:07:36|          2017-08-09 21:26:33|          2017-08-25 00:00:00|           cool_stuff|\n",
      "|7d19f4ef4d0446198...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-08-16 22:05:11| 58.9|        13.29|91a792fef70ecd8cc...|   delivered|     2017-08-10 21:48:40|2017-08-10 22:05:11|         2017-08-11 19:43:07|          2017-08-24 20:04:21|          2017-09-01 00:00:00|           cool_stuff|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM spark_late_items').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- order_delivered_carrier_date: string (nullable = true)\n",
      " |-- order_delivered_customer_date: string (nullable = true)\n",
      " |-- order_estimated_delivery_date: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_late_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 42 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_548/3231715687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark_late_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path/missed_shipping_limit_orders\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Bigdata\\spark\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m    953\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m--> 955\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Bigdata\\spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Bigdata\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Bigdata\\spark\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:839)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:240)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:605)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:240)\r\n\t... 42 more\r\n"
     ]
    }
   ],
   "source": [
    "spark_late_items.coalesce(1).write.option(\"header\", \"true\").csv(\"spark_late_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
